{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import  autograd,nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"Processed_train.csv\")\n",
    "sub = pd.read_csv(\"Processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.iloc[: ,1:]\n",
    "y = train_set[\"Survived\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1, 1, ..., 2, 0, 1],\n",
       "       [1, 0, 2, ..., 2, 0, 3],\n",
       "       [3, 0, 1, ..., 1, 1, 2],\n",
       "       ..., \n",
       "       [3, 0, 1, ..., 4, 0, 2],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [3, 1, 1, ..., 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\caiwei\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\caiwei\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\caiwei\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = scaler.transform(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 11)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "y_train = y_train.astype(\"float32\")\n",
    "X_test  = X_test.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "sub = sub.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import  data as gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = gdata.ArrayDataset(X_train,y_train)\n",
    "test_set = gdata.ArrayDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_iter = gdata.DataLoader(train_set,batch_size,shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.82389838  0.7333464   0.77569646 -0.47480127 -1.34116137 -0.58281463\n",
      "  -0.98057449 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [ 0.82389838  0.7333464  -1.61519468  0.76350826 -0.44369248  0.98308736\n",
      "   0.44113564 -0.54272044  0.06331877 -1.23483706  2.28917289]\n",
      " [ 0.82389838 -1.36361206 -0.41974914  2.00181794 -0.44369248 -0.58281463\n",
      "   2.40965748 -0.54272044  0.672548   -1.23483706  1.2922976 ]\n",
      " [-0.37327641 -1.36361206 -0.41974914 -0.47480127 -0.44369248  0.98308736\n",
      "   0.11304869 -0.54272044  0.06331877 -1.23483706  0.29542232]\n",
      " [ 0.82389838 -1.36361206 -0.41974914 -0.47480127 -1.34116137 -0.58281463\n",
      "   0.44113564 -0.54272044 -0.54591042  0.80982339  0.29542232]\n",
      " [ 0.82389838  0.7333464  -0.41974914 -0.47480127 -1.34116137 -0.58281463\n",
      "  -0.10567595 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [ 0.82389838  0.7333464  -0.41974914 -0.47480127 -0.44369248 -0.58281463\n",
      "  -0.21503827 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [-0.37327641 -1.36361206  1.97114205 -0.47480127 -0.44369248 -0.58281463\n",
      "  -1.08993685  1.84256935 -0.54591042  0.80982339  1.2922976 ]\n",
      " [-0.37327641  0.7333464  -0.41974914 -0.47480127 -0.44369248 -0.58281463\n",
      "  -0.87121218 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [-1.57045114 -1.36361206 -0.41974914 -0.47480127  1.35124528 -0.58281463\n",
      "  -0.65248752  1.84256935 -0.54591042  0.80982339  0.29542232]]\n",
      "<NDArray 10x11 @cpu(0)> \n",
      "[ 0.  1.  1.  1.  0.  0.  0.  0.  0.  1.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_iter:\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import  loss as gloss,nn\n",
    "from mxnet import gluon,init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 11\n",
    "num_outputs = 2\n",
    "W = nd.random.normal(scale=0.01,shape=(num_inputs,num_outputs))\n",
    "b = nd.zeros(num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义softmax()\n",
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1,keepdims = True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def net(X):\n",
    "    return softmax(nd.dot(X,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.82389838  0.7333464  -0.41974914 -0.47480127 -1.34116137 -0.58281463\n",
      "  -1.08993685 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [ 0.82389838  0.7333464  -0.41974914 -0.47480127 -1.34116137  0.98308736\n",
      "  -1.08993685 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [ 0.82389838 -1.36361206 -0.41974914 -0.47480127 -1.34116137  2.5489893\n",
      "   0.22241101 -0.54272044 -0.54591042  0.80982339  0.29542232]\n",
      " [ 0.82389838  0.7333464  -0.41974914 -0.47480127  1.35124528 -0.58281463\n",
      "  -1.63674843 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [-0.37327641  0.7333464   1.97114205  0.76350826  1.35124528 -0.58281463\n",
      "   0.65986031 -0.54272044  0.672548   -1.23483706 -0.70145297]\n",
      " [-0.37327641  0.7333464   1.97114205 -0.47480127 -0.44369248 -0.58281463\n",
      "  -0.10567595 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [-0.37327641 -1.36361206 -0.41974914 -0.47480127  0.45377639 -0.58281463\n",
      "   2.5190196  -0.54272044  0.06331877 -1.23483706  1.2922976 ]\n",
      " [ 0.82389838  0.7333464  -0.41974914 -0.47480127 -1.34116137 -0.58281463\n",
      "  -0.65248752 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [ 0.82389838  0.7333464  -0.41974914 -0.47480127 -0.44369248 -0.58281463\n",
      "  -0.87121218 -0.54272044 -0.54591042  0.80982339 -0.70145297]\n",
      " [-1.57045114  0.7333464  -0.41974914 -0.47480127  1.35124528 -0.58281463\n",
      "  -0.43376291  1.84256935  0.06331877 -1.23483706 -0.70145297]]\n",
      "<NDArray 10x11 @cpu(0)>\n",
      "\n",
      "[[ 0.49801001  0.50198996]\n",
      " [ 0.50427938  0.4957206 ]\n",
      " [ 0.52431673  0.47568321]\n",
      " [ 0.49912834  0.5008716 ]\n",
      " [ 0.48934776  0.51065224]\n",
      " [ 0.50026578  0.49973422]\n",
      " [ 0.49234101  0.50765896]\n",
      " [ 0.49605799  0.50394201]\n",
      " [ 0.49659342  0.50340658]\n",
      " [ 0.49791905  0.50208098]]\n",
      "<NDArray 10x2 @cpu(0)>\n",
      "\n",
      "[ 0.69713509  0.68462485  0.74300319  0.69489199  0.71468186  0.69261575\n",
      "  0.67794538  0.70106244  0.69998366  0.68899387]\n",
      "<NDArray 10 @cpu(0)>\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_iter:\n",
    "    print(X)\n",
    "    print(net(X))\n",
    "    print(cross_entropy(net(X),y))\n",
    "    print(accuracy(net(X),y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -nd.pick(y_hat,y).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 2.  2.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = nd.array([[0.1,0.3,0.6],[0.3,0.2,0.5]])\n",
    "y_hat.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算分类准确率\n",
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(axis=1)==y.astype(\"float32\")).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc = 0\n",
    "    for X,y in data_iter:\n",
    "        acc += accuracy(net(X),y)\n",
    "    return acc/len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34135803166362977"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_iter,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 2, loss 0.4354, train acc 0.808, test acc 0.860\n",
      "epoch 3, loss 0.4315, train acc 0.810, test acc 0.860\n",
      "epoch 4, loss 0.4431, train acc 0.807, test acc 0.861\n",
      "epoch 5, loss 0.4337, train acc 0.811, test acc 0.861\n",
      "epoch 6, loss 0.4351, train acc 0.807, test acc 0.861\n",
      "epoch 7, loss 0.4363, train acc 0.811, test acc 0.860\n",
      "epoch 8, loss 0.4353, train acc 0.806, test acc 0.861\n",
      "epoch 9, loss 0.4375, train acc 0.806, test acc 0.860\n",
      "epoch 10, loss 0.4466, train acc 0.803, test acc 0.861\n",
      "epoch 11, loss 0.4315, train acc 0.811, test acc 0.860\n",
      "epoch 12, loss 0.4389, train acc 0.806, test acc 0.860\n",
      "epoch 13, loss 0.4332, train acc 0.808, test acc 0.859\n",
      "epoch 14, loss 0.4298, train acc 0.811, test acc 0.860\n",
      "epoch 15, loss 0.4343, train acc 0.806, test acc 0.860\n",
      "epoch 16, loss 0.4416, train acc 0.799, test acc 0.860\n",
      "epoch 17, loss 0.4302, train acc 0.810, test acc 0.858\n",
      "epoch 18, loss 0.4328, train acc 0.808, test acc 0.860\n",
      "epoch 19, loss 0.4315, train acc 0.811, test acc 0.860\n",
      "epoch 20, loss 0.4307, train acc 0.811, test acc 0.860\n",
      "epoch 21, loss 0.4301, train acc 0.812, test acc 0.860\n",
      "epoch 22, loss 0.4298, train acc 0.814, test acc 0.860\n",
      "epoch 23, loss 0.4346, train acc 0.808, test acc 0.860\n",
      "epoch 24, loss 0.4301, train acc 0.811, test acc 0.860\n",
      "epoch 25, loss 0.4336, train acc 0.800, test acc 0.860\n",
      "epoch 26, loss 0.4297, train acc 0.812, test acc 0.860\n",
      "epoch 27, loss 0.4392, train acc 0.804, test acc 0.860\n",
      "epoch 28, loss 0.4314, train acc 0.812, test acc 0.861\n",
      "epoch 29, loss 0.4321, train acc 0.808, test acc 0.860\n",
      "epoch 30, loss 0.4364, train acc 0.808, test acc 0.860\n",
      "epoch 31, loss 0.4307, train acc 0.812, test acc 0.860\n",
      "epoch 32, loss 0.4303, train acc 0.812, test acc 0.860\n",
      "epoch 33, loss 0.4384, train acc 0.806, test acc 0.860\n",
      "epoch 34, loss 0.4463, train acc 0.801, test acc 0.861\n",
      "epoch 35, loss 0.4313, train acc 0.811, test acc 0.860\n",
      "epoch 36, loss 0.4303, train acc 0.811, test acc 0.861\n",
      "epoch 37, loss 0.4347, train acc 0.804, test acc 0.860\n",
      "epoch 38, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 39, loss 0.4295, train acc 0.814, test acc 0.860\n",
      "epoch 40, loss 0.4335, train acc 0.812, test acc 0.859\n",
      "epoch 41, loss 0.4345, train acc 0.812, test acc 0.861\n",
      "epoch 42, loss 0.4428, train acc 0.806, test acc 0.859\n",
      "epoch 43, loss 0.4301, train acc 0.811, test acc 0.860\n",
      "epoch 44, loss 0.4405, train acc 0.806, test acc 0.859\n",
      "epoch 45, loss 0.4318, train acc 0.810, test acc 0.861\n",
      "epoch 46, loss 0.4382, train acc 0.807, test acc 0.860\n",
      "epoch 47, loss 0.4298, train acc 0.808, test acc 0.860\n",
      "epoch 48, loss 0.4333, train acc 0.814, test acc 0.860\n",
      "epoch 49, loss 0.4321, train acc 0.810, test acc 0.861\n",
      "epoch 50, loss 0.4368, train acc 0.803, test acc 0.860\n",
      "epoch 51, loss 0.4295, train acc 0.810, test acc 0.860\n",
      "epoch 52, loss 0.4398, train acc 0.803, test acc 0.861\n",
      "epoch 53, loss 0.4310, train acc 0.811, test acc 0.860\n",
      "epoch 54, loss 0.4461, train acc 0.800, test acc 0.860\n",
      "epoch 55, loss 0.4492, train acc 0.800, test acc 0.860\n",
      "epoch 56, loss 0.4307, train acc 0.811, test acc 0.860\n",
      "epoch 57, loss 0.4335, train acc 0.810, test acc 0.861\n",
      "epoch 58, loss 0.4300, train acc 0.804, test acc 0.860\n",
      "epoch 59, loss 0.4321, train acc 0.814, test acc 0.860\n",
      "epoch 60, loss 0.4326, train acc 0.810, test acc 0.860\n",
      "epoch 61, loss 0.4390, train acc 0.807, test acc 0.859\n",
      "epoch 62, loss 0.4326, train acc 0.810, test acc 0.860\n",
      "epoch 63, loss 0.4455, train acc 0.804, test acc 0.859\n",
      "epoch 64, loss 0.4437, train acc 0.794, test acc 0.859\n",
      "epoch 65, loss 0.4363, train acc 0.804, test acc 0.860\n",
      "epoch 66, loss 0.4359, train acc 0.806, test acc 0.860\n",
      "epoch 67, loss 0.4313, train acc 0.812, test acc 0.860\n",
      "epoch 68, loss 0.4337, train acc 0.810, test acc 0.860\n",
      "epoch 69, loss 0.4301, train acc 0.811, test acc 0.859\n",
      "epoch 70, loss 0.4336, train acc 0.811, test acc 0.860\n",
      "epoch 71, loss 0.4424, train acc 0.806, test acc 0.865\n",
      "epoch 72, loss 0.4398, train acc 0.801, test acc 0.866\n",
      "epoch 73, loss 0.4307, train acc 0.811, test acc 0.861\n",
      "epoch 74, loss 0.4299, train acc 0.808, test acc 0.860\n",
      "epoch 75, loss 0.4299, train acc 0.812, test acc 0.860\n",
      "epoch 76, loss 0.4299, train acc 0.817, test acc 0.860\n",
      "epoch 77, loss 0.4336, train acc 0.812, test acc 0.865\n",
      "epoch 78, loss 0.4391, train acc 0.806, test acc 0.860\n",
      "epoch 79, loss 0.4297, train acc 0.815, test acc 0.860\n",
      "epoch 80, loss 0.4335, train acc 0.800, test acc 0.861\n",
      "epoch 81, loss 0.4436, train acc 0.806, test acc 0.860\n",
      "epoch 82, loss 0.4387, train acc 0.806, test acc 0.860\n",
      "epoch 83, loss 0.4298, train acc 0.812, test acc 0.861\n",
      "epoch 84, loss 0.4367, train acc 0.807, test acc 0.861\n",
      "epoch 85, loss 0.4314, train acc 0.807, test acc 0.861\n",
      "epoch 86, loss 0.4409, train acc 0.806, test acc 0.861\n",
      "epoch 87, loss 0.4301, train acc 0.811, test acc 0.860\n",
      "epoch 88, loss 0.4310, train acc 0.811, test acc 0.861\n",
      "epoch 89, loss 0.4330, train acc 0.812, test acc 0.860\n",
      "epoch 90, loss 0.4345, train acc 0.806, test acc 0.859\n",
      "epoch 91, loss 0.4301, train acc 0.810, test acc 0.860\n",
      "epoch 92, loss 0.4350, train acc 0.808, test acc 0.860\n",
      "epoch 93, loss 0.4305, train acc 0.811, test acc 0.861\n",
      "epoch 94, loss 0.4431, train acc 0.804, test acc 0.860\n",
      "epoch 95, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 96, loss 0.4298, train acc 0.814, test acc 0.860\n",
      "epoch 97, loss 0.4317, train acc 0.810, test acc 0.860\n",
      "epoch 98, loss 0.4294, train acc 0.812, test acc 0.860\n",
      "epoch 99, loss 0.4403, train acc 0.800, test acc 0.860\n",
      "epoch 100, loss 0.4310, train acc 0.812, test acc 0.860\n",
      "epoch 101, loss 0.4305, train acc 0.810, test acc 0.860\n",
      "epoch 102, loss 0.4306, train acc 0.812, test acc 0.860\n",
      "epoch 103, loss 0.4404, train acc 0.801, test acc 0.860\n",
      "epoch 104, loss 0.4425, train acc 0.806, test acc 0.861\n",
      "epoch 105, loss 0.4379, train acc 0.806, test acc 0.861\n",
      "epoch 106, loss 0.4300, train acc 0.812, test acc 0.859\n",
      "epoch 107, loss 0.4302, train acc 0.815, test acc 0.860\n",
      "epoch 108, loss 0.4300, train acc 0.811, test acc 0.860\n",
      "epoch 109, loss 0.4295, train acc 0.812, test acc 0.865\n",
      "epoch 110, loss 0.4351, train acc 0.806, test acc 0.860\n",
      "epoch 111, loss 0.4298, train acc 0.811, test acc 0.859\n",
      "epoch 112, loss 0.4301, train acc 0.812, test acc 0.861\n",
      "epoch 113, loss 0.4311, train acc 0.812, test acc 0.859\n",
      "epoch 114, loss 0.4300, train acc 0.812, test acc 0.861\n",
      "epoch 115, loss 0.4330, train acc 0.812, test acc 0.860\n",
      "epoch 116, loss 0.4339, train acc 0.811, test acc 0.860\n",
      "epoch 117, loss 0.4319, train acc 0.811, test acc 0.861\n",
      "epoch 118, loss 0.4375, train acc 0.804, test acc 0.861\n",
      "epoch 119, loss 0.4365, train acc 0.808, test acc 0.860\n",
      "epoch 120, loss 0.4308, train acc 0.812, test acc 0.859\n",
      "epoch 121, loss 0.4359, train acc 0.807, test acc 0.860\n",
      "epoch 122, loss 0.4484, train acc 0.797, test acc 0.860\n",
      "epoch 123, loss 0.4320, train acc 0.811, test acc 0.860\n",
      "epoch 124, loss 0.4333, train acc 0.810, test acc 0.861\n",
      "epoch 125, loss 0.4312, train acc 0.811, test acc 0.861\n",
      "epoch 126, loss 0.4325, train acc 0.811, test acc 0.860\n",
      "epoch 127, loss 0.4323, train acc 0.812, test acc 0.859\n",
      "epoch 128, loss 0.4304, train acc 0.811, test acc 0.860\n",
      "epoch 129, loss 0.4305, train acc 0.814, test acc 0.861\n",
      "epoch 130, loss 0.4436, train acc 0.803, test acc 0.861\n",
      "epoch 131, loss 0.4309, train acc 0.810, test acc 0.860\n",
      "epoch 132, loss 0.4368, train acc 0.807, test acc 0.861\n",
      "epoch 133, loss 0.4297, train acc 0.812, test acc 0.860\n",
      "epoch 134, loss 0.4335, train acc 0.814, test acc 0.860\n",
      "epoch 135, loss 0.4357, train acc 0.806, test acc 0.860\n",
      "epoch 136, loss 0.4308, train acc 0.807, test acc 0.860\n",
      "epoch 137, loss 0.4335, train acc 0.811, test acc 0.859\n",
      "epoch 138, loss 0.4318, train acc 0.812, test acc 0.861\n",
      "epoch 139, loss 0.4341, train acc 0.804, test acc 0.860\n",
      "epoch 140, loss 0.4318, train acc 0.811, test acc 0.860\n",
      "epoch 141, loss 0.4334, train acc 0.806, test acc 0.861\n",
      "epoch 142, loss 0.4373, train acc 0.804, test acc 0.861\n",
      "epoch 143, loss 0.4371, train acc 0.807, test acc 0.861\n",
      "epoch 144, loss 0.4297, train acc 0.811, test acc 0.860\n",
      "epoch 145, loss 0.4457, train acc 0.804, test acc 0.860\n",
      "epoch 146, loss 0.4389, train acc 0.806, test acc 0.860\n",
      "epoch 147, loss 0.4306, train acc 0.812, test acc 0.859\n",
      "epoch 148, loss 0.4425, train acc 0.808, test acc 0.860\n",
      "epoch 149, loss 0.4328, train acc 0.811, test acc 0.860\n",
      "epoch 150, loss 0.4378, train acc 0.806, test acc 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 151, loss 0.4314, train acc 0.812, test acc 0.860\n",
      "epoch 152, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 153, loss 0.4348, train acc 0.803, test acc 0.860\n",
      "epoch 154, loss 0.4346, train acc 0.807, test acc 0.860\n",
      "epoch 155, loss 0.4346, train acc 0.811, test acc 0.860\n",
      "epoch 156, loss 0.4328, train acc 0.811, test acc 0.860\n",
      "epoch 157, loss 0.4320, train acc 0.811, test acc 0.860\n",
      "epoch 158, loss 0.4306, train acc 0.810, test acc 0.860\n",
      "epoch 159, loss 0.4311, train acc 0.812, test acc 0.861\n",
      "epoch 160, loss 0.4309, train acc 0.812, test acc 0.860\n",
      "epoch 161, loss 0.4322, train acc 0.811, test acc 0.860\n",
      "epoch 162, loss 0.4300, train acc 0.811, test acc 0.859\n",
      "epoch 163, loss 0.4315, train acc 0.810, test acc 0.860\n",
      "epoch 164, loss 0.4297, train acc 0.811, test acc 0.860\n",
      "epoch 165, loss 0.4320, train acc 0.814, test acc 0.861\n",
      "epoch 166, loss 0.4313, train acc 0.810, test acc 0.861\n",
      "epoch 167, loss 0.4303, train acc 0.810, test acc 0.860\n",
      "epoch 168, loss 0.4314, train acc 0.808, test acc 0.861\n",
      "epoch 169, loss 0.4299, train acc 0.808, test acc 0.860\n",
      "epoch 170, loss 0.4316, train acc 0.814, test acc 0.861\n",
      "epoch 171, loss 0.4354, train acc 0.803, test acc 0.860\n",
      "epoch 172, loss 0.4401, train acc 0.806, test acc 0.861\n",
      "epoch 173, loss 0.4306, train acc 0.812, test acc 0.860\n",
      "epoch 174, loss 0.4326, train acc 0.811, test acc 0.859\n",
      "epoch 175, loss 0.4360, train acc 0.808, test acc 0.860\n",
      "epoch 176, loss 0.4385, train acc 0.804, test acc 0.861\n",
      "epoch 177, loss 0.4371, train acc 0.804, test acc 0.860\n",
      "epoch 178, loss 0.4304, train acc 0.811, test acc 0.861\n",
      "epoch 179, loss 0.4316, train acc 0.810, test acc 0.860\n",
      "epoch 180, loss 0.4303, train acc 0.810, test acc 0.861\n",
      "epoch 181, loss 0.4343, train acc 0.806, test acc 0.860\n",
      "epoch 182, loss 0.4328, train acc 0.810, test acc 0.860\n",
      "epoch 183, loss 0.4373, train acc 0.806, test acc 0.860\n",
      "epoch 184, loss 0.4329, train acc 0.812, test acc 0.861\n",
      "epoch 185, loss 0.4323, train acc 0.808, test acc 0.859\n",
      "epoch 186, loss 0.4330, train acc 0.810, test acc 0.860\n",
      "epoch 187, loss 0.4302, train acc 0.812, test acc 0.861\n",
      "epoch 188, loss 0.4330, train acc 0.814, test acc 0.860\n",
      "epoch 189, loss 0.4315, train acc 0.812, test acc 0.860\n",
      "epoch 190, loss 0.4301, train acc 0.812, test acc 0.861\n",
      "epoch 191, loss 0.4321, train acc 0.812, test acc 0.860\n",
      "epoch 192, loss 0.4305, train acc 0.811, test acc 0.861\n",
      "epoch 193, loss 0.4434, train acc 0.807, test acc 0.865\n",
      "epoch 194, loss 0.4324, train acc 0.810, test acc 0.860\n",
      "epoch 195, loss 0.4439, train acc 0.806, test acc 0.860\n",
      "epoch 196, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 197, loss 0.4304, train acc 0.811, test acc 0.861\n",
      "epoch 198, loss 0.4328, train acc 0.810, test acc 0.859\n",
      "epoch 199, loss 0.4297, train acc 0.811, test acc 0.860\n",
      "epoch 200, loss 0.4376, train acc 0.807, test acc 0.860\n",
      "epoch 201, loss 0.4440, train acc 0.804, test acc 0.859\n",
      "epoch 202, loss 0.4322, train acc 0.811, test acc 0.860\n",
      "epoch 203, loss 0.4312, train acc 0.810, test acc 0.860\n",
      "epoch 204, loss 0.4309, train acc 0.812, test acc 0.860\n",
      "epoch 205, loss 0.4301, train acc 0.810, test acc 0.865\n",
      "epoch 206, loss 0.4308, train acc 0.812, test acc 0.861\n",
      "epoch 207, loss 0.4337, train acc 0.808, test acc 0.860\n",
      "epoch 208, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 209, loss 0.4306, train acc 0.808, test acc 0.861\n",
      "epoch 210, loss 0.4377, train acc 0.806, test acc 0.861\n",
      "epoch 211, loss 0.4326, train acc 0.810, test acc 0.860\n",
      "epoch 212, loss 0.4321, train acc 0.811, test acc 0.861\n",
      "epoch 213, loss 0.4305, train acc 0.812, test acc 0.861\n",
      "epoch 214, loss 0.4363, train acc 0.807, test acc 0.860\n",
      "epoch 215, loss 0.4330, train acc 0.811, test acc 0.859\n",
      "epoch 216, loss 0.4333, train acc 0.815, test acc 0.860\n",
      "epoch 217, loss 0.4301, train acc 0.812, test acc 0.861\n",
      "epoch 218, loss 0.4316, train acc 0.810, test acc 0.861\n",
      "epoch 219, loss 0.4311, train acc 0.810, test acc 0.860\n",
      "epoch 220, loss 0.4318, train acc 0.811, test acc 0.859\n",
      "epoch 221, loss 0.4338, train acc 0.801, test acc 0.860\n",
      "epoch 222, loss 0.4322, train acc 0.811, test acc 0.861\n",
      "epoch 223, loss 0.4303, train acc 0.810, test acc 0.860\n",
      "epoch 224, loss 0.4317, train acc 0.810, test acc 0.860\n",
      "epoch 225, loss 0.4302, train acc 0.814, test acc 0.860\n",
      "epoch 226, loss 0.4311, train acc 0.808, test acc 0.859\n",
      "epoch 227, loss 0.4310, train acc 0.812, test acc 0.861\n",
      "epoch 228, loss 0.4332, train acc 0.807, test acc 0.859\n",
      "epoch 229, loss 0.4336, train acc 0.808, test acc 0.861\n",
      "epoch 230, loss 0.4312, train acc 0.811, test acc 0.861\n",
      "epoch 231, loss 0.4328, train acc 0.804, test acc 0.860\n",
      "epoch 232, loss 0.4304, train acc 0.810, test acc 0.860\n",
      "epoch 233, loss 0.4325, train acc 0.814, test acc 0.861\n",
      "epoch 234, loss 0.4325, train acc 0.812, test acc 0.860\n",
      "epoch 235, loss 0.4315, train acc 0.808, test acc 0.860\n",
      "epoch 236, loss 0.4318, train acc 0.811, test acc 0.860\n",
      "epoch 237, loss 0.4313, train acc 0.812, test acc 0.860\n",
      "epoch 238, loss 0.4315, train acc 0.811, test acc 0.860\n",
      "epoch 239, loss 0.4304, train acc 0.808, test acc 0.860\n",
      "epoch 240, loss 0.4308, train acc 0.814, test acc 0.861\n",
      "epoch 241, loss 0.4381, train acc 0.810, test acc 0.859\n",
      "epoch 242, loss 0.4312, train acc 0.806, test acc 0.860\n",
      "epoch 243, loss 0.4324, train acc 0.812, test acc 0.860\n",
      "epoch 244, loss 0.4311, train acc 0.808, test acc 0.859\n",
      "epoch 245, loss 0.4323, train acc 0.810, test acc 0.861\n",
      "epoch 246, loss 0.4360, train acc 0.808, test acc 0.861\n",
      "epoch 247, loss 0.4394, train acc 0.807, test acc 0.859\n",
      "epoch 248, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 249, loss 0.4348, train acc 0.806, test acc 0.860\n",
      "epoch 250, loss 0.4325, train acc 0.811, test acc 0.860\n",
      "epoch 251, loss 0.4310, train acc 0.811, test acc 0.860\n",
      "epoch 252, loss 0.4330, train acc 0.804, test acc 0.861\n",
      "epoch 253, loss 0.4309, train acc 0.812, test acc 0.860\n",
      "epoch 254, loss 0.4344, train acc 0.800, test acc 0.861\n",
      "epoch 255, loss 0.4348, train acc 0.803, test acc 0.860\n",
      "epoch 256, loss 0.4301, train acc 0.810, test acc 0.860\n",
      "epoch 257, loss 0.4370, train acc 0.806, test acc 0.861\n",
      "epoch 258, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 259, loss 0.4376, train acc 0.806, test acc 0.860\n",
      "epoch 260, loss 0.4297, train acc 0.811, test acc 0.860\n",
      "epoch 261, loss 0.4386, train acc 0.800, test acc 0.861\n",
      "epoch 262, loss 0.4370, train acc 0.803, test acc 0.860\n",
      "epoch 263, loss 0.4378, train acc 0.807, test acc 0.860\n",
      "epoch 264, loss 0.4299, train acc 0.811, test acc 0.860\n",
      "epoch 265, loss 0.4332, train acc 0.812, test acc 0.860\n",
      "epoch 266, loss 0.4408, train acc 0.807, test acc 0.861\n",
      "epoch 267, loss 0.4297, train acc 0.814, test acc 0.860\n",
      "epoch 268, loss 0.4301, train acc 0.815, test acc 0.861\n",
      "epoch 269, loss 0.4441, train acc 0.806, test acc 0.860\n",
      "epoch 270, loss 0.4344, train acc 0.807, test acc 0.861\n",
      "epoch 271, loss 0.4331, train acc 0.811, test acc 0.861\n",
      "epoch 272, loss 0.4308, train acc 0.811, test acc 0.860\n",
      "epoch 273, loss 0.4377, train acc 0.804, test acc 0.860\n",
      "epoch 274, loss 0.4301, train acc 0.811, test acc 0.860\n",
      "epoch 275, loss 0.4324, train acc 0.811, test acc 0.860\n",
      "epoch 276, loss 0.4309, train acc 0.810, test acc 0.860\n",
      "epoch 277, loss 0.4357, train acc 0.806, test acc 0.859\n",
      "epoch 278, loss 0.4298, train acc 0.811, test acc 0.860\n",
      "epoch 279, loss 0.4319, train acc 0.811, test acc 0.860\n",
      "epoch 280, loss 0.4328, train acc 0.811, test acc 0.860\n",
      "epoch 281, loss 0.4323, train acc 0.811, test acc 0.860\n",
      "epoch 282, loss 0.4443, train acc 0.803, test acc 0.861\n",
      "epoch 283, loss 0.4301, train acc 0.812, test acc 0.860\n",
      "epoch 284, loss 0.4314, train acc 0.811, test acc 0.859\n",
      "epoch 285, loss 0.4312, train acc 0.810, test acc 0.860\n",
      "epoch 286, loss 0.4309, train acc 0.811, test acc 0.860\n",
      "epoch 287, loss 0.4372, train acc 0.806, test acc 0.861\n",
      "epoch 288, loss 0.4308, train acc 0.811, test acc 0.860\n",
      "epoch 289, loss 0.4347, train acc 0.812, test acc 0.860\n",
      "epoch 290, loss 0.4385, train acc 0.804, test acc 0.860\n",
      "epoch 291, loss 0.4311, train acc 0.814, test acc 0.861\n",
      "epoch 292, loss 0.4384, train acc 0.807, test acc 0.860\n",
      "epoch 293, loss 0.4318, train acc 0.811, test acc 0.860\n",
      "epoch 294, loss 0.4343, train acc 0.801, test acc 0.859\n",
      "epoch 295, loss 0.4300, train acc 0.811, test acc 0.860\n",
      "epoch 296, loss 0.4382, train acc 0.806, test acc 0.860\n",
      "epoch 297, loss 0.4351, train acc 0.806, test acc 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 298, loss 0.4306, train acc 0.811, test acc 0.861\n",
      "epoch 299, loss 0.4394, train acc 0.800, test acc 0.860\n",
      "epoch 300, loss 0.4330, train acc 0.810, test acc 0.861\n",
      "epoch 301, loss 0.4404, train acc 0.804, test acc 0.861\n",
      "epoch 302, loss 0.4299, train acc 0.808, test acc 0.860\n",
      "epoch 303, loss 0.4308, train acc 0.808, test acc 0.860\n",
      "epoch 304, loss 0.4390, train acc 0.808, test acc 0.860\n",
      "epoch 305, loss 0.4423, train acc 0.806, test acc 0.859\n",
      "epoch 306, loss 0.4310, train acc 0.810, test acc 0.860\n",
      "epoch 307, loss 0.4361, train acc 0.803, test acc 0.859\n",
      "epoch 308, loss 0.4322, train acc 0.808, test acc 0.860\n",
      "epoch 309, loss 0.4301, train acc 0.812, test acc 0.860\n",
      "epoch 310, loss 0.4309, train acc 0.812, test acc 0.860\n",
      "epoch 311, loss 0.4312, train acc 0.810, test acc 0.861\n",
      "epoch 312, loss 0.4297, train acc 0.812, test acc 0.860\n",
      "epoch 313, loss 0.4356, train acc 0.808, test acc 0.860\n",
      "epoch 314, loss 0.4325, train acc 0.814, test acc 0.859\n",
      "epoch 315, loss 0.4349, train acc 0.804, test acc 0.860\n",
      "epoch 316, loss 0.4350, train acc 0.806, test acc 0.861\n",
      "epoch 317, loss 0.4314, train acc 0.810, test acc 0.859\n",
      "epoch 318, loss 0.4549, train acc 0.800, test acc 0.860\n",
      "epoch 319, loss 0.4396, train acc 0.807, test acc 0.861\n",
      "epoch 320, loss 0.4358, train acc 0.814, test acc 0.860\n",
      "epoch 321, loss 0.4308, train acc 0.815, test acc 0.861\n",
      "epoch 322, loss 0.4324, train acc 0.810, test acc 0.860\n",
      "epoch 323, loss 0.4444, train acc 0.806, test acc 0.860\n",
      "epoch 324, loss 0.4323, train acc 0.810, test acc 0.860\n",
      "epoch 325, loss 0.4297, train acc 0.812, test acc 0.861\n",
      "epoch 326, loss 0.4336, train acc 0.808, test acc 0.860\n",
      "epoch 327, loss 0.4369, train acc 0.807, test acc 0.860\n",
      "epoch 328, loss 0.4323, train acc 0.811, test acc 0.860\n",
      "epoch 329, loss 0.4355, train acc 0.808, test acc 0.860\n",
      "epoch 330, loss 0.4328, train acc 0.810, test acc 0.860\n",
      "epoch 331, loss 0.4363, train acc 0.806, test acc 0.860\n",
      "epoch 332, loss 0.4298, train acc 0.811, test acc 0.859\n",
      "epoch 333, loss 0.4303, train acc 0.812, test acc 0.860\n",
      "epoch 334, loss 0.4331, train acc 0.807, test acc 0.860\n",
      "epoch 335, loss 0.4344, train acc 0.811, test acc 0.860\n",
      "epoch 336, loss 0.4366, train acc 0.806, test acc 0.859\n",
      "epoch 337, loss 0.4325, train acc 0.812, test acc 0.860\n",
      "epoch 338, loss 0.4449, train acc 0.804, test acc 0.861\n",
      "epoch 339, loss 0.4385, train acc 0.806, test acc 0.860\n",
      "epoch 340, loss 0.4309, train acc 0.815, test acc 0.860\n",
      "epoch 341, loss 0.4311, train acc 0.812, test acc 0.860\n",
      "epoch 342, loss 0.4346, train acc 0.806, test acc 0.860\n",
      "epoch 343, loss 0.4411, train acc 0.806, test acc 0.866\n",
      "epoch 344, loss 0.4372, train acc 0.810, test acc 0.860\n",
      "epoch 345, loss 0.4348, train acc 0.810, test acc 0.861\n",
      "epoch 346, loss 0.4312, train acc 0.814, test acc 0.860\n",
      "epoch 347, loss 0.4300, train acc 0.815, test acc 0.860\n",
      "epoch 348, loss 0.4310, train acc 0.810, test acc 0.866\n",
      "epoch 349, loss 0.4303, train acc 0.811, test acc 0.859\n",
      "epoch 350, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 351, loss 0.4331, train acc 0.812, test acc 0.861\n",
      "epoch 352, loss 0.4307, train acc 0.810, test acc 0.860\n",
      "epoch 353, loss 0.4385, train acc 0.804, test acc 0.860\n",
      "epoch 354, loss 0.4454, train acc 0.800, test acc 0.860\n",
      "epoch 355, loss 0.4297, train acc 0.810, test acc 0.860\n",
      "epoch 356, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 357, loss 0.4303, train acc 0.811, test acc 0.861\n",
      "epoch 358, loss 0.4296, train acc 0.808, test acc 0.861\n",
      "epoch 359, loss 0.4352, train acc 0.806, test acc 0.860\n",
      "epoch 360, loss 0.4310, train acc 0.811, test acc 0.859\n",
      "epoch 361, loss 0.4315, train acc 0.810, test acc 0.860\n",
      "epoch 362, loss 0.4429, train acc 0.801, test acc 0.861\n",
      "epoch 363, loss 0.4302, train acc 0.811, test acc 0.860\n",
      "epoch 364, loss 0.4332, train acc 0.808, test acc 0.860\n",
      "epoch 365, loss 0.4348, train acc 0.814, test acc 0.865\n",
      "epoch 366, loss 0.4351, train acc 0.808, test acc 0.866\n",
      "epoch 367, loss 0.4356, train acc 0.804, test acc 0.861\n",
      "epoch 368, loss 0.4417, train acc 0.806, test acc 0.860\n",
      "epoch 369, loss 0.4314, train acc 0.811, test acc 0.859\n",
      "epoch 370, loss 0.4309, train acc 0.814, test acc 0.859\n",
      "epoch 371, loss 0.4306, train acc 0.811, test acc 0.866\n",
      "epoch 372, loss 0.4420, train acc 0.804, test acc 0.859\n",
      "epoch 373, loss 0.4321, train acc 0.813, test acc 0.859\n",
      "epoch 374, loss 0.4311, train acc 0.812, test acc 0.861\n",
      "epoch 375, loss 0.4313, train acc 0.810, test acc 0.861\n",
      "epoch 376, loss 0.4373, train acc 0.807, test acc 0.859\n",
      "epoch 377, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 378, loss 0.4318, train acc 0.810, test acc 0.861\n",
      "epoch 379, loss 0.4301, train acc 0.810, test acc 0.860\n",
      "epoch 380, loss 0.4379, train acc 0.804, test acc 0.860\n",
      "epoch 381, loss 0.4340, train acc 0.811, test acc 0.860\n",
      "epoch 382, loss 0.4321, train acc 0.808, test acc 0.861\n",
      "epoch 383, loss 0.4374, train acc 0.808, test acc 0.861\n",
      "epoch 384, loss 0.4333, train acc 0.811, test acc 0.860\n",
      "epoch 385, loss 0.4340, train acc 0.812, test acc 0.861\n",
      "epoch 386, loss 0.4425, train acc 0.804, test acc 0.860\n",
      "epoch 387, loss 0.4294, train acc 0.810, test acc 0.860\n",
      "epoch 388, loss 0.4378, train acc 0.806, test acc 0.859\n",
      "epoch 389, loss 0.4332, train acc 0.814, test acc 0.860\n",
      "epoch 390, loss 0.4303, train acc 0.814, test acc 0.859\n",
      "epoch 391, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 392, loss 0.4300, train acc 0.808, test acc 0.861\n",
      "epoch 393, loss 0.4313, train acc 0.812, test acc 0.859\n",
      "epoch 394, loss 0.4301, train acc 0.811, test acc 0.859\n",
      "epoch 395, loss 0.4503, train acc 0.801, test acc 0.860\n",
      "epoch 396, loss 0.4316, train acc 0.812, test acc 0.860\n",
      "epoch 397, loss 0.4382, train acc 0.806, test acc 0.860\n",
      "epoch 398, loss 0.4311, train acc 0.810, test acc 0.860\n",
      "epoch 399, loss 0.4300, train acc 0.812, test acc 0.861\n",
      "epoch 400, loss 0.4320, train acc 0.811, test acc 0.860\n",
      "epoch 401, loss 0.4368, train acc 0.806, test acc 0.860\n",
      "epoch 402, loss 0.4303, train acc 0.811, test acc 0.861\n",
      "epoch 403, loss 0.4297, train acc 0.814, test acc 0.860\n",
      "epoch 404, loss 0.4298, train acc 0.815, test acc 0.861\n",
      "epoch 405, loss 0.4462, train acc 0.803, test acc 0.859\n",
      "epoch 406, loss 0.4297, train acc 0.810, test acc 0.861\n",
      "epoch 407, loss 0.4302, train acc 0.810, test acc 0.860\n",
      "epoch 408, loss 0.4308, train acc 0.810, test acc 0.860\n",
      "epoch 409, loss 0.4383, train acc 0.808, test acc 0.861\n",
      "epoch 410, loss 0.4353, train acc 0.804, test acc 0.859\n",
      "epoch 411, loss 0.4378, train acc 0.807, test acc 0.861\n",
      "epoch 412, loss 0.4306, train acc 0.807, test acc 0.861\n",
      "epoch 413, loss 0.4322, train acc 0.812, test acc 0.860\n",
      "epoch 414, loss 0.4295, train acc 0.811, test acc 0.860\n",
      "epoch 415, loss 0.4362, train acc 0.808, test acc 0.860\n",
      "epoch 416, loss 0.4299, train acc 0.812, test acc 0.860\n",
      "epoch 417, loss 0.4296, train acc 0.812, test acc 0.860\n",
      "epoch 418, loss 0.4365, train acc 0.807, test acc 0.861\n",
      "epoch 419, loss 0.4298, train acc 0.812, test acc 0.861\n",
      "epoch 420, loss 0.4357, train acc 0.810, test acc 0.860\n",
      "epoch 421, loss 0.4332, train acc 0.807, test acc 0.860\n",
      "epoch 422, loss 0.4321, train acc 0.812, test acc 0.859\n",
      "epoch 423, loss 0.4500, train acc 0.803, test acc 0.860\n",
      "epoch 424, loss 0.4399, train acc 0.801, test acc 0.860\n",
      "epoch 425, loss 0.4306, train acc 0.808, test acc 0.860\n",
      "epoch 426, loss 0.4306, train acc 0.806, test acc 0.861\n",
      "epoch 427, loss 0.4307, train acc 0.810, test acc 0.860\n",
      "epoch 428, loss 0.4315, train acc 0.808, test acc 0.860\n",
      "epoch 429, loss 0.4325, train acc 0.812, test acc 0.860\n",
      "epoch 430, loss 0.4502, train acc 0.800, test acc 0.860\n",
      "epoch 431, loss 0.4326, train acc 0.812, test acc 0.860\n",
      "epoch 432, loss 0.4353, train acc 0.806, test acc 0.860\n",
      "epoch 433, loss 0.4323, train acc 0.814, test acc 0.860\n",
      "epoch 434, loss 0.4340, train acc 0.806, test acc 0.860\n",
      "epoch 435, loss 0.4323, train acc 0.810, test acc 0.860\n",
      "epoch 436, loss 0.4347, train acc 0.808, test acc 0.861\n",
      "epoch 437, loss 0.4422, train acc 0.803, test acc 0.860\n",
      "epoch 438, loss 0.4425, train acc 0.804, test acc 0.860\n",
      "epoch 439, loss 0.4360, train acc 0.806, test acc 0.860\n",
      "epoch 440, loss 0.4305, train acc 0.811, test acc 0.861\n",
      "epoch 441, loss 0.4364, train acc 0.806, test acc 0.861\n",
      "epoch 442, loss 0.4299, train acc 0.811, test acc 0.861\n",
      "epoch 443, loss 0.4385, train acc 0.804, test acc 0.861\n",
      "epoch 444, loss 0.4309, train acc 0.812, test acc 0.860\n",
      "epoch 445, loss 0.4430, train acc 0.807, test acc 0.859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 446, loss 0.4303, train acc 0.810, test acc 0.860\n",
      "epoch 447, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 448, loss 0.4350, train acc 0.811, test acc 0.860\n",
      "epoch 449, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 450, loss 0.4295, train acc 0.815, test acc 0.860\n",
      "epoch 451, loss 0.4297, train acc 0.808, test acc 0.860\n",
      "epoch 452, loss 0.4312, train acc 0.808, test acc 0.860\n",
      "epoch 453, loss 0.4321, train acc 0.808, test acc 0.861\n",
      "epoch 454, loss 0.4318, train acc 0.808, test acc 0.861\n",
      "epoch 455, loss 0.4383, train acc 0.807, test acc 0.861\n",
      "epoch 456, loss 0.4295, train acc 0.811, test acc 0.860\n",
      "epoch 457, loss 0.4315, train acc 0.808, test acc 0.859\n",
      "epoch 458, loss 0.4310, train acc 0.812, test acc 0.860\n",
      "epoch 459, loss 0.4325, train acc 0.808, test acc 0.861\n",
      "epoch 460, loss 0.4399, train acc 0.807, test acc 0.860\n",
      "epoch 461, loss 0.4303, train acc 0.814, test acc 0.860\n",
      "epoch 462, loss 0.4319, train acc 0.811, test acc 0.860\n",
      "epoch 463, loss 0.4394, train acc 0.797, test acc 0.861\n",
      "epoch 464, loss 0.4381, train acc 0.801, test acc 0.860\n",
      "epoch 465, loss 0.4306, train acc 0.810, test acc 0.860\n",
      "epoch 466, loss 0.4308, train acc 0.807, test acc 0.860\n",
      "epoch 467, loss 0.4435, train acc 0.794, test acc 0.860\n",
      "epoch 468, loss 0.4324, train acc 0.808, test acc 0.859\n",
      "epoch 469, loss 0.4356, train acc 0.806, test acc 0.861\n",
      "epoch 470, loss 0.4294, train acc 0.810, test acc 0.860\n",
      "epoch 471, loss 0.4301, train acc 0.807, test acc 0.859\n",
      "epoch 472, loss 0.4428, train acc 0.806, test acc 0.860\n",
      "epoch 473, loss 0.4303, train acc 0.811, test acc 0.860\n",
      "epoch 474, loss 0.4327, train acc 0.810, test acc 0.860\n",
      "epoch 475, loss 0.4381, train acc 0.807, test acc 0.861\n",
      "epoch 476, loss 0.4340, train acc 0.806, test acc 0.861\n",
      "epoch 477, loss 0.4304, train acc 0.807, test acc 0.859\n",
      "epoch 478, loss 0.4296, train acc 0.811, test acc 0.860\n",
      "epoch 479, loss 0.4446, train acc 0.807, test acc 0.860\n",
      "epoch 480, loss 0.4300, train acc 0.811, test acc 0.860\n",
      "epoch 481, loss 0.4358, train acc 0.804, test acc 0.861\n",
      "epoch 482, loss 0.4377, train acc 0.806, test acc 0.859\n",
      "epoch 483, loss 0.4299, train acc 0.811, test acc 0.860\n",
      "epoch 484, loss 0.4417, train acc 0.804, test acc 0.859\n",
      "epoch 485, loss 0.4314, train acc 0.808, test acc 0.860\n",
      "epoch 486, loss 0.4357, train acc 0.806, test acc 0.860\n",
      "epoch 487, loss 0.4425, train acc 0.811, test acc 0.860\n",
      "epoch 488, loss 0.4337, train acc 0.814, test acc 0.860\n",
      "epoch 489, loss 0.4321, train acc 0.810, test acc 0.860\n",
      "epoch 490, loss 0.4324, train acc 0.814, test acc 0.860\n",
      "epoch 491, loss 0.4336, train acc 0.812, test acc 0.859\n",
      "epoch 492, loss 0.4307, train acc 0.810, test acc 0.860\n",
      "epoch 493, loss 0.4316, train acc 0.811, test acc 0.860\n",
      "epoch 494, loss 0.4338, train acc 0.807, test acc 0.860\n",
      "epoch 495, loss 0.4317, train acc 0.811, test acc 0.860\n",
      "epoch 496, loss 0.4382, train acc 0.807, test acc 0.860\n",
      "epoch 497, loss 0.4309, train acc 0.811, test acc 0.860\n",
      "epoch 498, loss 0.4327, train acc 0.812, test acc 0.861\n",
      "epoch 499, loss 0.4315, train acc 0.811, test acc 0.861\n",
      "epoch 500, loss 0.4305, train acc 0.810, test acc 0.861\n",
      "epoch 501, loss 0.4397, train acc 0.804, test acc 0.860\n",
      "epoch 502, loss 0.4316, train acc 0.810, test acc 0.860\n",
      "epoch 503, loss 0.4374, train acc 0.806, test acc 0.858\n",
      "epoch 504, loss 0.4309, train acc 0.814, test acc 0.859\n",
      "epoch 505, loss 0.4339, train acc 0.810, test acc 0.861\n",
      "epoch 506, loss 0.4303, train acc 0.812, test acc 0.860\n",
      "epoch 507, loss 0.4412, train acc 0.803, test acc 0.859\n",
      "epoch 508, loss 0.4314, train acc 0.812, test acc 0.860\n",
      "epoch 509, loss 0.4374, train acc 0.806, test acc 0.861\n",
      "epoch 510, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 511, loss 0.4309, train acc 0.810, test acc 0.861\n",
      "epoch 512, loss 0.4325, train acc 0.810, test acc 0.859\n",
      "epoch 513, loss 0.4362, train acc 0.804, test acc 0.861\n",
      "epoch 514, loss 0.4298, train acc 0.810, test acc 0.859\n",
      "epoch 515, loss 0.4319, train acc 0.812, test acc 0.861\n",
      "epoch 516, loss 0.4305, train acc 0.812, test acc 0.860\n",
      "epoch 517, loss 0.4372, train acc 0.807, test acc 0.860\n",
      "epoch 518, loss 0.4438, train acc 0.804, test acc 0.860\n",
      "epoch 519, loss 0.4371, train acc 0.808, test acc 0.860\n",
      "epoch 520, loss 0.4311, train acc 0.808, test acc 0.860\n",
      "epoch 521, loss 0.4301, train acc 0.806, test acc 0.861\n",
      "epoch 522, loss 0.4302, train acc 0.814, test acc 0.860\n",
      "epoch 523, loss 0.4323, train acc 0.810, test acc 0.861\n",
      "epoch 524, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 525, loss 0.4316, train acc 0.814, test acc 0.860\n",
      "epoch 526, loss 0.4307, train acc 0.812, test acc 0.860\n",
      "epoch 527, loss 0.4298, train acc 0.812, test acc 0.860\n",
      "epoch 528, loss 0.4453, train acc 0.806, test acc 0.861\n",
      "epoch 529, loss 0.4349, train acc 0.808, test acc 0.860\n",
      "epoch 530, loss 0.4394, train acc 0.807, test acc 0.859\n",
      "epoch 531, loss 0.4316, train acc 0.810, test acc 0.861\n",
      "epoch 532, loss 0.4300, train acc 0.808, test acc 0.860\n",
      "epoch 533, loss 0.4396, train acc 0.803, test acc 0.860\n",
      "epoch 534, loss 0.4302, train acc 0.811, test acc 0.860\n",
      "epoch 535, loss 0.4326, train acc 0.808, test acc 0.861\n",
      "epoch 536, loss 0.4298, train acc 0.808, test acc 0.860\n",
      "epoch 537, loss 0.4320, train acc 0.812, test acc 0.860\n",
      "epoch 538, loss 0.4323, train acc 0.810, test acc 0.860\n",
      "epoch 539, loss 0.4306, train acc 0.811, test acc 0.860\n",
      "epoch 540, loss 0.4332, train acc 0.811, test acc 0.861\n",
      "epoch 541, loss 0.4437, train acc 0.807, test acc 0.861\n",
      "epoch 542, loss 0.4344, train acc 0.807, test acc 0.860\n",
      "epoch 543, loss 0.4319, train acc 0.811, test acc 0.860\n",
      "epoch 544, loss 0.4320, train acc 0.814, test acc 0.861\n",
      "epoch 545, loss 0.4370, train acc 0.806, test acc 0.860\n",
      "epoch 546, loss 0.4321, train acc 0.814, test acc 0.860\n",
      "epoch 547, loss 0.4332, train acc 0.810, test acc 0.861\n",
      "epoch 548, loss 0.4459, train acc 0.799, test acc 0.860\n",
      "epoch 549, loss 0.4348, train acc 0.808, test acc 0.861\n",
      "epoch 550, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 551, loss 0.4399, train acc 0.801, test acc 0.861\n",
      "epoch 552, loss 0.4342, train acc 0.810, test acc 0.861\n",
      "epoch 553, loss 0.4298, train acc 0.814, test acc 0.859\n",
      "epoch 554, loss 0.4310, train acc 0.814, test acc 0.860\n",
      "epoch 555, loss 0.4320, train acc 0.815, test acc 0.860\n",
      "epoch 556, loss 0.4303, train acc 0.810, test acc 0.861\n",
      "epoch 557, loss 0.4300, train acc 0.811, test acc 0.860\n",
      "epoch 558, loss 0.4349, train acc 0.804, test acc 0.861\n",
      "epoch 559, loss 0.4347, train acc 0.801, test acc 0.860\n",
      "epoch 560, loss 0.4303, train acc 0.814, test acc 0.860\n",
      "epoch 561, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 562, loss 0.4335, train acc 0.814, test acc 0.859\n",
      "epoch 563, loss 0.4486, train acc 0.803, test acc 0.859\n",
      "epoch 564, loss 0.4302, train acc 0.814, test acc 0.860\n",
      "epoch 565, loss 0.4306, train acc 0.811, test acc 0.860\n",
      "epoch 566, loss 0.4296, train acc 0.808, test acc 0.861\n",
      "epoch 567, loss 0.4357, train acc 0.811, test acc 0.858\n",
      "epoch 568, loss 0.4355, train acc 0.806, test acc 0.861\n",
      "epoch 569, loss 0.4351, train acc 0.807, test acc 0.860\n",
      "epoch 570, loss 0.4345, train acc 0.808, test acc 0.860\n",
      "epoch 571, loss 0.4318, train acc 0.811, test acc 0.860\n",
      "epoch 572, loss 0.4372, train acc 0.806, test acc 0.860\n",
      "epoch 573, loss 0.4425, train acc 0.810, test acc 0.860\n",
      "epoch 574, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 575, loss 0.4297, train acc 0.808, test acc 0.860\n",
      "epoch 576, loss 0.4307, train acc 0.815, test acc 0.860\n",
      "epoch 577, loss 0.4314, train acc 0.812, test acc 0.860\n",
      "epoch 578, loss 0.4430, train acc 0.804, test acc 0.860\n",
      "epoch 579, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 580, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 581, loss 0.4323, train acc 0.807, test acc 0.860\n",
      "epoch 582, loss 0.4424, train acc 0.804, test acc 0.860\n",
      "epoch 583, loss 0.4339, train acc 0.808, test acc 0.860\n",
      "epoch 584, loss 0.4369, train acc 0.807, test acc 0.860\n",
      "epoch 585, loss 0.4328, train acc 0.811, test acc 0.860\n",
      "epoch 586, loss 0.4376, train acc 0.806, test acc 0.860\n",
      "epoch 587, loss 0.4299, train acc 0.810, test acc 0.861\n",
      "epoch 588, loss 0.4324, train acc 0.814, test acc 0.860\n",
      "epoch 589, loss 0.4346, train acc 0.807, test acc 0.860\n",
      "epoch 590, loss 0.4300, train acc 0.812, test acc 0.860\n",
      "epoch 591, loss 0.4320, train acc 0.811, test acc 0.860\n",
      "epoch 592, loss 0.4305, train acc 0.814, test acc 0.860\n",
      "epoch 593, loss 0.4300, train acc 0.811, test acc 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 594, loss 0.4319, train acc 0.810, test acc 0.861\n",
      "epoch 595, loss 0.4307, train acc 0.811, test acc 0.860\n",
      "epoch 596, loss 0.4327, train acc 0.808, test acc 0.861\n",
      "epoch 597, loss 0.4332, train acc 0.811, test acc 0.860\n",
      "epoch 598, loss 0.4417, train acc 0.801, test acc 0.861\n",
      "epoch 599, loss 0.4302, train acc 0.811, test acc 0.861\n",
      "epoch 600, loss 0.4295, train acc 0.812, test acc 0.860\n",
      "epoch 601, loss 0.4453, train acc 0.806, test acc 0.861\n",
      "epoch 602, loss 0.4301, train acc 0.811, test acc 0.859\n",
      "epoch 603, loss 0.4299, train acc 0.811, test acc 0.867\n",
      "epoch 604, loss 0.4408, train acc 0.804, test acc 0.860\n",
      "epoch 605, loss 0.4318, train acc 0.811, test acc 0.859\n",
      "epoch 606, loss 0.4304, train acc 0.811, test acc 0.859\n",
      "epoch 607, loss 0.4309, train acc 0.811, test acc 0.860\n",
      "epoch 608, loss 0.4314, train acc 0.814, test acc 0.860\n",
      "epoch 609, loss 0.4302, train acc 0.811, test acc 0.860\n",
      "epoch 610, loss 0.4361, train acc 0.804, test acc 0.860\n",
      "epoch 611, loss 0.4429, train acc 0.808, test acc 0.861\n",
      "epoch 612, loss 0.4325, train acc 0.810, test acc 0.861\n",
      "epoch 613, loss 0.4316, train acc 0.812, test acc 0.860\n",
      "epoch 614, loss 0.4453, train acc 0.806, test acc 0.861\n",
      "epoch 615, loss 0.4323, train acc 0.810, test acc 0.859\n",
      "epoch 616, loss 0.4327, train acc 0.814, test acc 0.860\n",
      "epoch 617, loss 0.4304, train acc 0.811, test acc 0.860\n",
      "epoch 618, loss 0.4336, train acc 0.811, test acc 0.860\n",
      "epoch 619, loss 0.4301, train acc 0.810, test acc 0.860\n",
      "epoch 620, loss 0.4574, train acc 0.803, test acc 0.859\n",
      "epoch 621, loss 0.4345, train acc 0.806, test acc 0.861\n",
      "epoch 622, loss 0.4302, train acc 0.810, test acc 0.860\n",
      "epoch 623, loss 0.4305, train acc 0.812, test acc 0.859\n",
      "epoch 624, loss 0.4457, train acc 0.799, test acc 0.861\n",
      "epoch 625, loss 0.4340, train acc 0.812, test acc 0.860\n",
      "epoch 626, loss 0.4339, train acc 0.812, test acc 0.859\n",
      "epoch 627, loss 0.4297, train acc 0.812, test acc 0.860\n",
      "epoch 628, loss 0.4401, train acc 0.806, test acc 0.860\n",
      "epoch 629, loss 0.4324, train acc 0.808, test acc 0.860\n",
      "epoch 630, loss 0.4303, train acc 0.810, test acc 0.860\n",
      "epoch 631, loss 0.4346, train acc 0.806, test acc 0.860\n",
      "epoch 632, loss 0.4327, train acc 0.812, test acc 0.861\n",
      "epoch 633, loss 0.4331, train acc 0.811, test acc 0.860\n",
      "epoch 634, loss 0.4361, train acc 0.804, test acc 0.860\n",
      "epoch 635, loss 0.4339, train acc 0.811, test acc 0.860\n",
      "epoch 636, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 637, loss 0.4300, train acc 0.812, test acc 0.860\n",
      "epoch 638, loss 0.4299, train acc 0.808, test acc 0.860\n",
      "epoch 639, loss 0.4309, train acc 0.811, test acc 0.859\n",
      "epoch 640, loss 0.4305, train acc 0.811, test acc 0.860\n",
      "epoch 641, loss 0.4368, train acc 0.806, test acc 0.861\n",
      "epoch 642, loss 0.4300, train acc 0.808, test acc 0.860\n",
      "epoch 643, loss 0.4338, train acc 0.807, test acc 0.861\n",
      "epoch 644, loss 0.4514, train acc 0.800, test acc 0.860\n",
      "epoch 645, loss 0.4302, train acc 0.810, test acc 0.861\n",
      "epoch 646, loss 0.4315, train acc 0.808, test acc 0.859\n",
      "epoch 647, loss 0.4433, train acc 0.806, test acc 0.860\n",
      "epoch 648, loss 0.4319, train acc 0.811, test acc 0.859\n",
      "epoch 649, loss 0.4302, train acc 0.808, test acc 0.860\n",
      "epoch 650, loss 0.4340, train acc 0.810, test acc 0.861\n",
      "epoch 651, loss 0.4305, train acc 0.810, test acc 0.860\n",
      "epoch 652, loss 0.4306, train acc 0.808, test acc 0.860\n",
      "epoch 653, loss 0.4340, train acc 0.806, test acc 0.859\n",
      "epoch 654, loss 0.4438, train acc 0.803, test acc 0.860\n",
      "epoch 655, loss 0.4323, train acc 0.808, test acc 0.861\n",
      "epoch 656, loss 0.4419, train acc 0.800, test acc 0.859\n",
      "epoch 657, loss 0.4301, train acc 0.812, test acc 0.860\n",
      "epoch 658, loss 0.4307, train acc 0.812, test acc 0.861\n",
      "epoch 659, loss 0.4498, train acc 0.799, test acc 0.861\n",
      "epoch 660, loss 0.4408, train acc 0.807, test acc 0.859\n",
      "epoch 661, loss 0.4363, train acc 0.807, test acc 0.859\n",
      "epoch 662, loss 0.4408, train acc 0.807, test acc 0.861\n",
      "epoch 663, loss 0.4352, train acc 0.806, test acc 0.859\n",
      "epoch 664, loss 0.4335, train acc 0.804, test acc 0.860\n",
      "epoch 665, loss 0.4300, train acc 0.814, test acc 0.860\n",
      "epoch 666, loss 0.4405, train acc 0.806, test acc 0.865\n",
      "epoch 667, loss 0.4331, train acc 0.811, test acc 0.860\n",
      "epoch 668, loss 0.4354, train acc 0.808, test acc 0.860\n",
      "epoch 669, loss 0.4301, train acc 0.808, test acc 0.859\n",
      "epoch 670, loss 0.4334, train acc 0.807, test acc 0.860\n",
      "epoch 671, loss 0.4303, train acc 0.811, test acc 0.861\n",
      "epoch 672, loss 0.4344, train acc 0.803, test acc 0.861\n",
      "epoch 673, loss 0.4301, train acc 0.814, test acc 0.860\n",
      "epoch 674, loss 0.4343, train acc 0.814, test acc 0.860\n",
      "epoch 675, loss 0.4335, train acc 0.811, test acc 0.861\n",
      "epoch 676, loss 0.4298, train acc 0.808, test acc 0.861\n",
      "epoch 677, loss 0.4307, train acc 0.812, test acc 0.860\n",
      "epoch 678, loss 0.4307, train acc 0.812, test acc 0.860\n",
      "epoch 679, loss 0.4310, train acc 0.812, test acc 0.861\n",
      "epoch 680, loss 0.4326, train acc 0.811, test acc 0.859\n",
      "epoch 681, loss 0.4300, train acc 0.812, test acc 0.860\n",
      "epoch 682, loss 0.4334, train acc 0.812, test acc 0.860\n",
      "epoch 683, loss 0.4462, train acc 0.803, test acc 0.860\n",
      "epoch 684, loss 0.4301, train acc 0.810, test acc 0.860\n",
      "epoch 685, loss 0.4304, train acc 0.811, test acc 0.860\n",
      "epoch 686, loss 0.4369, train acc 0.804, test acc 0.860\n",
      "epoch 687, loss 0.4346, train acc 0.808, test acc 0.859\n",
      "epoch 688, loss 0.4308, train acc 0.811, test acc 0.860\n",
      "epoch 689, loss 0.4328, train acc 0.810, test acc 0.861\n",
      "epoch 690, loss 0.4340, train acc 0.804, test acc 0.860\n",
      "epoch 691, loss 0.4338, train acc 0.806, test acc 0.859\n",
      "epoch 692, loss 0.4350, train acc 0.807, test acc 0.866\n",
      "epoch 693, loss 0.4328, train acc 0.808, test acc 0.860\n",
      "epoch 694, loss 0.4372, train acc 0.806, test acc 0.860\n",
      "epoch 695, loss 0.4347, train acc 0.806, test acc 0.860\n",
      "epoch 696, loss 0.4342, train acc 0.803, test acc 0.860\n",
      "epoch 697, loss 0.4357, train acc 0.806, test acc 0.860\n",
      "epoch 698, loss 0.4322, train acc 0.810, test acc 0.859\n",
      "epoch 699, loss 0.4367, train acc 0.806, test acc 0.859\n",
      "epoch 700, loss 0.4311, train acc 0.811, test acc 0.858\n",
      "epoch 701, loss 0.4316, train acc 0.815, test acc 0.861\n",
      "epoch 702, loss 0.4417, train acc 0.806, test acc 0.860\n",
      "epoch 703, loss 0.4469, train acc 0.801, test acc 0.860\n",
      "epoch 704, loss 0.4300, train acc 0.808, test acc 0.861\n",
      "epoch 705, loss 0.4318, train acc 0.808, test acc 0.860\n",
      "epoch 706, loss 0.4354, train acc 0.806, test acc 0.860\n",
      "epoch 707, loss 0.4306, train acc 0.808, test acc 0.859\n",
      "epoch 708, loss 0.4346, train acc 0.803, test acc 0.860\n",
      "epoch 709, loss 0.4382, train acc 0.806, test acc 0.861\n",
      "epoch 710, loss 0.4320, train acc 0.812, test acc 0.860\n",
      "epoch 711, loss 0.4360, train acc 0.806, test acc 0.860\n",
      "epoch 712, loss 0.4323, train acc 0.811, test acc 0.860\n",
      "epoch 713, loss 0.4448, train acc 0.796, test acc 0.860\n",
      "epoch 714, loss 0.4303, train acc 0.810, test acc 0.860\n",
      "epoch 715, loss 0.4303, train acc 0.810, test acc 0.861\n",
      "epoch 716, loss 0.4298, train acc 0.812, test acc 0.861\n",
      "epoch 717, loss 0.4317, train acc 0.811, test acc 0.861\n",
      "epoch 718, loss 0.4303, train acc 0.811, test acc 0.860\n",
      "epoch 719, loss 0.4342, train acc 0.804, test acc 0.860\n",
      "epoch 720, loss 0.4377, train acc 0.807, test acc 0.860\n",
      "epoch 721, loss 0.4318, train acc 0.817, test acc 0.860\n",
      "epoch 722, loss 0.4359, train acc 0.806, test acc 0.860\n",
      "epoch 723, loss 0.4327, train acc 0.807, test acc 0.860\n",
      "epoch 724, loss 0.4380, train acc 0.806, test acc 0.860\n",
      "epoch 725, loss 0.4301, train acc 0.811, test acc 0.860\n",
      "epoch 726, loss 0.4311, train acc 0.812, test acc 0.861\n",
      "epoch 727, loss 0.4342, train acc 0.804, test acc 0.861\n",
      "epoch 728, loss 0.4419, train acc 0.799, test acc 0.860\n",
      "epoch 729, loss 0.4455, train acc 0.808, test acc 0.860\n",
      "epoch 730, loss 0.4299, train acc 0.810, test acc 0.859\n",
      "epoch 731, loss 0.4351, train acc 0.807, test acc 0.861\n",
      "epoch 732, loss 0.4295, train acc 0.814, test acc 0.860\n",
      "epoch 733, loss 0.4453, train acc 0.807, test acc 0.861\n",
      "epoch 734, loss 0.4308, train acc 0.815, test acc 0.859\n",
      "epoch 735, loss 0.4435, train acc 0.808, test acc 0.861\n",
      "epoch 736, loss 0.4314, train acc 0.808, test acc 0.860\n",
      "epoch 737, loss 0.4381, train acc 0.806, test acc 0.860\n",
      "epoch 738, loss 0.4311, train acc 0.811, test acc 0.860\n",
      "epoch 739, loss 0.4304, train acc 0.810, test acc 0.860\n",
      "epoch 740, loss 0.4301, train acc 0.808, test acc 0.859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 741, loss 0.4343, train acc 0.807, test acc 0.860\n",
      "epoch 742, loss 0.4320, train acc 0.808, test acc 0.860\n",
      "epoch 743, loss 0.4387, train acc 0.804, test acc 0.861\n",
      "epoch 744, loss 0.4309, train acc 0.811, test acc 0.860\n",
      "epoch 745, loss 0.4342, train acc 0.808, test acc 0.860\n",
      "epoch 746, loss 0.4296, train acc 0.808, test acc 0.861\n",
      "epoch 747, loss 0.4302, train acc 0.815, test acc 0.860\n",
      "epoch 748, loss 0.4319, train acc 0.807, test acc 0.860\n",
      "epoch 749, loss 0.4303, train acc 0.812, test acc 0.860\n",
      "epoch 750, loss 0.4329, train acc 0.811, test acc 0.860\n",
      "epoch 751, loss 0.4331, train acc 0.814, test acc 0.861\n",
      "epoch 752, loss 0.4319, train acc 0.812, test acc 0.860\n",
      "epoch 753, loss 0.4296, train acc 0.812, test acc 0.860\n",
      "epoch 754, loss 0.4330, train acc 0.812, test acc 0.867\n",
      "epoch 755, loss 0.4380, train acc 0.807, test acc 0.860\n",
      "epoch 756, loss 0.4332, train acc 0.810, test acc 0.860\n",
      "epoch 757, loss 0.4463, train acc 0.800, test acc 0.861\n",
      "epoch 758, loss 0.4328, train acc 0.810, test acc 0.860\n",
      "epoch 759, loss 0.4313, train acc 0.806, test acc 0.860\n",
      "epoch 760, loss 0.4296, train acc 0.812, test acc 0.860\n",
      "epoch 761, loss 0.4324, train acc 0.810, test acc 0.861\n",
      "epoch 762, loss 0.4311, train acc 0.811, test acc 0.861\n",
      "epoch 763, loss 0.4317, train acc 0.807, test acc 0.860\n",
      "epoch 764, loss 0.4321, train acc 0.811, test acc 0.860\n",
      "epoch 765, loss 0.4303, train acc 0.812, test acc 0.866\n",
      "epoch 766, loss 0.4391, train acc 0.810, test acc 0.867\n",
      "epoch 767, loss 0.4393, train acc 0.797, test acc 0.859\n",
      "epoch 768, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 769, loss 0.4320, train acc 0.814, test acc 0.859\n",
      "epoch 770, loss 0.4295, train acc 0.811, test acc 0.860\n",
      "epoch 771, loss 0.4322, train acc 0.812, test acc 0.860\n",
      "epoch 772, loss 0.4312, train acc 0.808, test acc 0.860\n",
      "epoch 773, loss 0.4334, train acc 0.808, test acc 0.861\n",
      "epoch 774, loss 0.4340, train acc 0.807, test acc 0.860\n",
      "epoch 775, loss 0.4295, train acc 0.811, test acc 0.860\n",
      "epoch 776, loss 0.4308, train acc 0.812, test acc 0.860\n",
      "epoch 777, loss 0.4333, train acc 0.812, test acc 0.860\n",
      "epoch 778, loss 0.4385, train acc 0.806, test acc 0.860\n",
      "epoch 779, loss 0.4455, train acc 0.804, test acc 0.861\n",
      "epoch 780, loss 0.4300, train acc 0.806, test acc 0.860\n",
      "epoch 781, loss 0.4368, train acc 0.804, test acc 0.860\n",
      "epoch 782, loss 0.4311, train acc 0.808, test acc 0.860\n",
      "epoch 783, loss 0.4324, train acc 0.808, test acc 0.861\n",
      "epoch 784, loss 0.4332, train acc 0.811, test acc 0.860\n",
      "epoch 785, loss 0.4376, train acc 0.806, test acc 0.860\n",
      "epoch 786, loss 0.4304, train acc 0.810, test acc 0.860\n",
      "epoch 787, loss 0.4304, train acc 0.810, test acc 0.860\n",
      "epoch 788, loss 0.4336, train acc 0.808, test acc 0.860\n",
      "epoch 789, loss 0.4314, train acc 0.812, test acc 0.860\n",
      "epoch 790, loss 0.4346, train acc 0.807, test acc 0.860\n",
      "epoch 791, loss 0.4298, train acc 0.811, test acc 0.861\n",
      "epoch 792, loss 0.4305, train acc 0.814, test acc 0.860\n",
      "epoch 793, loss 0.4312, train acc 0.811, test acc 0.859\n",
      "epoch 794, loss 0.4306, train acc 0.811, test acc 0.860\n",
      "epoch 795, loss 0.4308, train acc 0.812, test acc 0.860\n",
      "epoch 796, loss 0.4380, train acc 0.806, test acc 0.860\n",
      "epoch 797, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 798, loss 0.4305, train acc 0.812, test acc 0.866\n",
      "epoch 799, loss 0.4328, train acc 0.811, test acc 0.861\n",
      "epoch 800, loss 0.4301, train acc 0.807, test acc 0.861\n",
      "epoch 801, loss 0.4306, train acc 0.810, test acc 0.867\n",
      "epoch 802, loss 0.4303, train acc 0.812, test acc 0.859\n",
      "epoch 803, loss 0.4316, train acc 0.810, test acc 0.861\n",
      "epoch 804, loss 0.4327, train acc 0.814, test acc 0.861\n",
      "epoch 805, loss 0.4335, train acc 0.803, test acc 0.860\n",
      "epoch 806, loss 0.4329, train acc 0.814, test acc 0.860\n",
      "epoch 807, loss 0.4306, train acc 0.811, test acc 0.861\n",
      "epoch 808, loss 0.4331, train acc 0.811, test acc 0.859\n",
      "epoch 809, loss 0.4399, train acc 0.800, test acc 0.859\n",
      "epoch 810, loss 0.4380, train acc 0.808, test acc 0.860\n",
      "epoch 811, loss 0.4310, train acc 0.808, test acc 0.861\n",
      "epoch 812, loss 0.4299, train acc 0.814, test acc 0.860\n",
      "epoch 813, loss 0.4304, train acc 0.811, test acc 0.861\n",
      "epoch 814, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 815, loss 0.4328, train acc 0.808, test acc 0.861\n",
      "epoch 816, loss 0.4468, train acc 0.803, test acc 0.859\n",
      "epoch 817, loss 0.4364, train acc 0.804, test acc 0.861\n",
      "epoch 818, loss 0.4407, train acc 0.811, test acc 0.860\n",
      "epoch 819, loss 0.4506, train acc 0.797, test acc 0.860\n",
      "epoch 820, loss 0.4343, train acc 0.808, test acc 0.859\n",
      "epoch 821, loss 0.4297, train acc 0.808, test acc 0.860\n",
      "epoch 822, loss 0.4296, train acc 0.810, test acc 0.860\n",
      "epoch 823, loss 0.4325, train acc 0.810, test acc 0.860\n",
      "epoch 824, loss 0.4312, train acc 0.811, test acc 0.860\n",
      "epoch 825, loss 0.4294, train acc 0.815, test acc 0.861\n",
      "epoch 826, loss 0.4298, train acc 0.814, test acc 0.861\n",
      "epoch 827, loss 0.4308, train acc 0.811, test acc 0.860\n",
      "epoch 828, loss 0.4301, train acc 0.811, test acc 0.861\n",
      "epoch 829, loss 0.4320, train acc 0.812, test acc 0.860\n",
      "epoch 830, loss 0.4317, train acc 0.807, test acc 0.861\n",
      "epoch 831, loss 0.4316, train acc 0.812, test acc 0.860\n",
      "epoch 832, loss 0.4451, train acc 0.806, test acc 0.860\n",
      "epoch 833, loss 0.4434, train acc 0.808, test acc 0.860\n",
      "epoch 834, loss 0.4357, train acc 0.811, test acc 0.861\n",
      "epoch 835, loss 0.4306, train acc 0.812, test acc 0.860\n",
      "epoch 836, loss 0.4330, train acc 0.811, test acc 0.861\n",
      "epoch 837, loss 0.4320, train acc 0.811, test acc 0.861\n",
      "epoch 838, loss 0.4313, train acc 0.810, test acc 0.861\n",
      "epoch 839, loss 0.4458, train acc 0.800, test acc 0.859\n",
      "epoch 840, loss 0.4479, train acc 0.806, test acc 0.861\n",
      "epoch 841, loss 0.4329, train acc 0.814, test acc 0.861\n",
      "epoch 842, loss 0.4305, train acc 0.808, test acc 0.860\n",
      "epoch 843, loss 0.4459, train acc 0.806, test acc 0.860\n",
      "epoch 844, loss 0.4375, train acc 0.804, test acc 0.860\n",
      "epoch 845, loss 0.4314, train acc 0.811, test acc 0.860\n",
      "epoch 846, loss 0.4385, train acc 0.804, test acc 0.860\n",
      "epoch 847, loss 0.4307, train acc 0.814, test acc 0.860\n",
      "epoch 848, loss 0.4302, train acc 0.812, test acc 0.860\n",
      "epoch 849, loss 0.4306, train acc 0.810, test acc 0.866\n",
      "epoch 850, loss 0.4315, train acc 0.810, test acc 0.861\n",
      "epoch 851, loss 0.4364, train acc 0.804, test acc 0.860\n",
      "epoch 852, loss 0.4359, train acc 0.807, test acc 0.861\n",
      "epoch 853, loss 0.4302, train acc 0.811, test acc 0.860\n",
      "epoch 854, loss 0.4319, train acc 0.808, test acc 0.860\n",
      "epoch 855, loss 0.4375, train acc 0.806, test acc 0.860\n",
      "epoch 856, loss 0.4353, train acc 0.804, test acc 0.860\n",
      "epoch 857, loss 0.4314, train acc 0.808, test acc 0.861\n",
      "epoch 858, loss 0.4356, train acc 0.804, test acc 0.860\n",
      "epoch 859, loss 0.4393, train acc 0.807, test acc 0.861\n",
      "epoch 860, loss 0.4351, train acc 0.804, test acc 0.860\n",
      "epoch 861, loss 0.4311, train acc 0.811, test acc 0.859\n",
      "epoch 862, loss 0.4340, train acc 0.811, test acc 0.860\n",
      "epoch 863, loss 0.4340, train acc 0.804, test acc 0.861\n",
      "epoch 864, loss 0.4327, train acc 0.814, test acc 0.861\n",
      "epoch 865, loss 0.4527, train acc 0.803, test acc 0.860\n",
      "epoch 866, loss 0.4324, train acc 0.812, test acc 0.860\n",
      "epoch 867, loss 0.4327, train acc 0.812, test acc 0.860\n",
      "epoch 868, loss 0.4297, train acc 0.808, test acc 0.860\n",
      "epoch 869, loss 0.4337, train acc 0.814, test acc 0.859\n",
      "epoch 870, loss 0.4325, train acc 0.812, test acc 0.860\n",
      "epoch 871, loss 0.4315, train acc 0.808, test acc 0.866\n",
      "epoch 872, loss 0.4305, train acc 0.814, test acc 0.860\n",
      "epoch 873, loss 0.4412, train acc 0.808, test acc 0.859\n",
      "epoch 874, loss 0.4332, train acc 0.804, test acc 0.860\n",
      "epoch 875, loss 0.4341, train acc 0.803, test acc 0.860\n",
      "epoch 876, loss 0.4439, train acc 0.807, test acc 0.860\n",
      "epoch 877, loss 0.4307, train acc 0.812, test acc 0.859\n",
      "epoch 878, loss 0.4331, train acc 0.812, test acc 0.859\n",
      "epoch 879, loss 0.4327, train acc 0.812, test acc 0.860\n",
      "epoch 880, loss 0.4460, train acc 0.797, test acc 0.860\n",
      "epoch 881, loss 0.4327, train acc 0.811, test acc 0.860\n",
      "epoch 882, loss 0.4297, train acc 0.811, test acc 0.860\n",
      "epoch 883, loss 0.4307, train acc 0.808, test acc 0.860\n",
      "epoch 884, loss 0.4380, train acc 0.808, test acc 0.859\n",
      "epoch 885, loss 0.4427, train acc 0.807, test acc 0.861\n",
      "epoch 886, loss 0.4311, train acc 0.815, test acc 0.860\n",
      "epoch 887, loss 0.4346, train acc 0.803, test acc 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 888, loss 0.4337, train acc 0.807, test acc 0.861\n",
      "epoch 889, loss 0.4385, train acc 0.801, test acc 0.859\n",
      "epoch 890, loss 0.4407, train acc 0.808, test acc 0.860\n",
      "epoch 891, loss 0.4336, train acc 0.814, test acc 0.860\n",
      "epoch 892, loss 0.4323, train acc 0.815, test acc 0.860\n",
      "epoch 893, loss 0.4579, train acc 0.797, test acc 0.859\n",
      "epoch 894, loss 0.4316, train acc 0.807, test acc 0.859\n",
      "epoch 895, loss 0.4298, train acc 0.812, test acc 0.860\n",
      "epoch 896, loss 0.4310, train acc 0.808, test acc 0.861\n",
      "epoch 897, loss 0.4312, train acc 0.812, test acc 0.860\n",
      "epoch 898, loss 0.4299, train acc 0.811, test acc 0.860\n",
      "epoch 899, loss 0.4315, train acc 0.808, test acc 0.861\n",
      "epoch 900, loss 0.4318, train acc 0.808, test acc 0.860\n",
      "epoch 901, loss 0.4316, train acc 0.810, test acc 0.860\n",
      "epoch 902, loss 0.4304, train acc 0.811, test acc 0.861\n",
      "epoch 903, loss 0.4360, train acc 0.806, test acc 0.860\n",
      "epoch 904, loss 0.4308, train acc 0.811, test acc 0.859\n",
      "epoch 905, loss 0.4309, train acc 0.810, test acc 0.860\n",
      "epoch 906, loss 0.4362, train acc 0.807, test acc 0.861\n",
      "epoch 907, loss 0.4313, train acc 0.811, test acc 0.861\n",
      "epoch 908, loss 0.4332, train acc 0.808, test acc 0.860\n",
      "epoch 909, loss 0.4335, train acc 0.815, test acc 0.860\n",
      "epoch 910, loss 0.4394, train acc 0.806, test acc 0.860\n",
      "epoch 911, loss 0.4323, train acc 0.812, test acc 0.861\n",
      "epoch 912, loss 0.4470, train acc 0.807, test acc 0.859\n",
      "epoch 913, loss 0.4306, train acc 0.811, test acc 0.860\n",
      "epoch 914, loss 0.4333, train acc 0.808, test acc 0.859\n",
      "epoch 915, loss 0.4437, train acc 0.808, test acc 0.861\n",
      "epoch 916, loss 0.4462, train acc 0.806, test acc 0.861\n",
      "epoch 917, loss 0.4332, train acc 0.814, test acc 0.859\n",
      "epoch 918, loss 0.4375, train acc 0.807, test acc 0.860\n",
      "epoch 919, loss 0.4322, train acc 0.808, test acc 0.861\n",
      "epoch 920, loss 0.4298, train acc 0.812, test acc 0.860\n",
      "epoch 921, loss 0.4344, train acc 0.810, test acc 0.860\n",
      "epoch 922, loss 0.4345, train acc 0.806, test acc 0.860\n",
      "epoch 923, loss 0.4456, train acc 0.804, test acc 0.860\n",
      "epoch 924, loss 0.4328, train acc 0.810, test acc 0.861\n",
      "epoch 925, loss 0.4320, train acc 0.814, test acc 0.860\n",
      "epoch 926, loss 0.4305, train acc 0.814, test acc 0.860\n",
      "epoch 927, loss 0.4394, train acc 0.804, test acc 0.860\n",
      "epoch 928, loss 0.4397, train acc 0.807, test acc 0.861\n",
      "epoch 929, loss 0.4368, train acc 0.806, test acc 0.861\n",
      "epoch 930, loss 0.4306, train acc 0.811, test acc 0.861\n",
      "epoch 931, loss 0.4320, train acc 0.812, test acc 0.861\n",
      "epoch 932, loss 0.4316, train acc 0.810, test acc 0.860\n",
      "epoch 933, loss 0.4294, train acc 0.811, test acc 0.860\n",
      "epoch 934, loss 0.4314, train acc 0.814, test acc 0.861\n",
      "epoch 935, loss 0.4311, train acc 0.814, test acc 0.860\n",
      "epoch 936, loss 0.4322, train acc 0.810, test acc 0.860\n",
      "epoch 937, loss 0.4359, train acc 0.804, test acc 0.860\n",
      "epoch 938, loss 0.4302, train acc 0.814, test acc 0.861\n",
      "epoch 939, loss 0.4314, train acc 0.811, test acc 0.860\n",
      "epoch 940, loss 0.4305, train acc 0.811, test acc 0.861\n",
      "epoch 941, loss 0.4383, train acc 0.807, test acc 0.860\n",
      "epoch 942, loss 0.4316, train acc 0.811, test acc 0.860\n",
      "epoch 943, loss 0.4298, train acc 0.808, test acc 0.860\n",
      "epoch 944, loss 0.4354, train acc 0.804, test acc 0.860\n",
      "epoch 945, loss 0.4303, train acc 0.810, test acc 0.860\n",
      "epoch 946, loss 0.4334, train acc 0.808, test acc 0.860\n",
      "epoch 947, loss 0.4379, train acc 0.806, test acc 0.859\n",
      "epoch 948, loss 0.4307, train acc 0.811, test acc 0.858\n",
      "epoch 949, loss 0.4306, train acc 0.811, test acc 0.860\n",
      "epoch 950, loss 0.4377, train acc 0.807, test acc 0.860\n",
      "epoch 951, loss 0.4307, train acc 0.814, test acc 0.859\n",
      "epoch 952, loss 0.4323, train acc 0.812, test acc 0.861\n",
      "epoch 953, loss 0.4332, train acc 0.811, test acc 0.860\n",
      "epoch 954, loss 0.4345, train acc 0.807, test acc 0.860\n",
      "epoch 955, loss 0.4383, train acc 0.806, test acc 0.860\n",
      "epoch 956, loss 0.4339, train acc 0.811, test acc 0.861\n",
      "epoch 957, loss 0.4307, train acc 0.811, test acc 0.861\n",
      "epoch 958, loss 0.4381, train acc 0.808, test acc 0.860\n",
      "epoch 959, loss 0.4312, train acc 0.808, test acc 0.861\n",
      "epoch 960, loss 0.4316, train acc 0.811, test acc 0.861\n",
      "epoch 961, loss 0.4327, train acc 0.810, test acc 0.861\n",
      "epoch 962, loss 0.4302, train acc 0.810, test acc 0.859\n",
      "epoch 963, loss 0.4316, train acc 0.811, test acc 0.866\n",
      "epoch 964, loss 0.4324, train acc 0.810, test acc 0.860\n",
      "epoch 965, loss 0.4301, train acc 0.810, test acc 0.860\n",
      "epoch 966, loss 0.4322, train acc 0.814, test acc 0.861\n",
      "epoch 967, loss 0.4425, train acc 0.806, test acc 0.860\n",
      "epoch 968, loss 0.4420, train acc 0.808, test acc 0.861\n",
      "epoch 969, loss 0.4344, train acc 0.810, test acc 0.860\n",
      "epoch 970, loss 0.4304, train acc 0.812, test acc 0.860\n",
      "epoch 971, loss 0.4299, train acc 0.810, test acc 0.861\n",
      "epoch 972, loss 0.4391, train acc 0.807, test acc 0.861\n",
      "epoch 973, loss 0.4413, train acc 0.804, test acc 0.860\n",
      "epoch 974, loss 0.4354, train acc 0.804, test acc 0.861\n",
      "epoch 975, loss 0.4440, train acc 0.806, test acc 0.859\n",
      "epoch 976, loss 0.4338, train acc 0.806, test acc 0.859\n",
      "epoch 977, loss 0.4347, train acc 0.804, test acc 0.860\n",
      "epoch 978, loss 0.4312, train acc 0.810, test acc 0.860\n",
      "epoch 979, loss 0.4340, train acc 0.811, test acc 0.861\n",
      "epoch 980, loss 0.4300, train acc 0.811, test acc 0.860\n",
      "epoch 981, loss 0.4306, train acc 0.811, test acc 0.860\n",
      "epoch 982, loss 0.4383, train acc 0.806, test acc 0.860\n",
      "epoch 983, loss 0.4320, train acc 0.810, test acc 0.860\n",
      "epoch 984, loss 0.4348, train acc 0.806, test acc 0.860\n",
      "epoch 985, loss 0.4315, train acc 0.811, test acc 0.860\n",
      "epoch 986, loss 0.4321, train acc 0.811, test acc 0.861\n",
      "epoch 987, loss 0.4295, train acc 0.808, test acc 0.860\n",
      "epoch 988, loss 0.4338, train acc 0.806, test acc 0.859\n",
      "epoch 989, loss 0.4396, train acc 0.803, test acc 0.859\n",
      "epoch 990, loss 0.4300, train acc 0.812, test acc 0.859\n",
      "epoch 991, loss 0.4300, train acc 0.812, test acc 0.859\n",
      "epoch 992, loss 0.4310, train acc 0.811, test acc 0.859\n",
      "epoch 993, loss 0.4574, train acc 0.801, test acc 0.859\n",
      "epoch 994, loss 0.4354, train acc 0.808, test acc 0.860\n",
      "epoch 995, loss 0.4335, train acc 0.811, test acc 0.860\n",
      "epoch 996, loss 0.4317, train acc 0.812, test acc 0.860\n",
      "epoch 997, loss 0.4303, train acc 0.811, test acc 0.860\n",
      "epoch 998, loss 0.4332, train acc 0.812, test acc 0.861\n",
      "epoch 999, loss 0.4325, train acc 0.814, test acc 0.860\n",
      "epoch 1000, loss 0.4310, train acc 0.812, test acc 0.860\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 100, 0.001\n",
    "\n",
    "# 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum = 0\n",
    "        train_acc_sum = 0\n",
    "        for X, y in train_iter:\n",
    "#             print(X,y)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y)\n",
    "#                 print(l)\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                gb.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)  # 下一节将用到。\n",
    "            train_l_sum += l.mean().asscalar()\n",
    "            train_acc_sum += accuracy(y_hat, y)\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / len(train_iter),\n",
    "                 train_acc_sum / len(train_iter), test_acc))\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs,\n",
    "          batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = nd.array(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "        0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,\n",
       "        1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,\n",
       "        1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "        1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
       "        0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
       "        1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,\n",
       "        0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,\n",
       "        1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "        0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,\n",
       "        0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(sub).argmax(axis=1).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         1\n",
       "19           911         1\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         0\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         1\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         0\n",
       "405         1297         1\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         1\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         1\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\"PassengerId\":pd.read_csv(\"test.csv\")[\"PassengerId\"],\n",
    "                          \"Survived\":net(sub).argmax(axis=1).asnumpy().astype(\"int\")\n",
    "                          })\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_mxnet.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
